---
title: "Enrichir les donn√©es de Wikidata avec des informations provenant de Mastodon"
author: "Damien Belv√®ze"
date: 20250208
output: 
  html_document:
    toc: true
    toc-depth: 2
    theme: cerulean
css: style.css
bibliography: biblio.bib
csl: nature.csl
editor_options: 
  markdown: 
    wrap: sentence
---

# introduction

Les donn√©es sur les esp√®ces biologiques, √† l'instar de toutes les autres donn√©es de recherche doivent √™tre FAIR (Findable, Accessible, Interoperable, Reusable).
Depuis 2008, la fondation √† but non lucratif Plazi Verein qui r√©unit des chercheurs et chercheuses dans le domaine de la biodiversit√© et des professionnels de l'information maintiennent une banque de donn√©es sur les esp√®ces, la \*Treatment Bank" Cette base de donn√©es librement accessible sur le web contient des informations sur un nombre croissant d'esp√®ces et accorde √† chacune un identifiant unique et p√©renne qui permet de les identifier sans ambiguit√© et sur le temps long.

Depuis quelques temps, Plazi tient un compte sur le F√©divers (Mastodon) et y publie des informations sur les derni√®res esp√®ces.
Ces informations comprennent entre autres informations le nom de l'esp√®ce, le lieu de d√©couverte, l'identifiant de l'esp√®ce dans la base de donn√©es Plazi et la publication dans laquelle cette d√©couverte est mentionn√©e.

<img src="images/harlequin_frog.png" alt="un toot (message) de Mastodon annon√ßant la d√©couverte d&apos;une nouvelle grenouille Harlequin en Chine" width="500">

Le F√©divers est un r√©seau structur√© par un protocole de communication (ActivityPub) qui permet de faire communiquer plusieurs serveurs entre eux ; il s'agit d'une architecture d√©centralis√©e √† laquelle les utilisateurs peuvent avoir acc√®s au moyen de logiciels orient√©s vers le partage de vid√©os (PeerTube), de photos (Pixelfeld) ou de messages (Diaspora et Mastodon).
Ces logiciels se pr√©sentent comme des alternatives transparentes, libres, √©thiques et d√©centralis√©es aux plateformes qui, selon C√©dric Durand, constituent le fer de lance du techno-f√©odalisme (√† savoir r√©spectivement Youtube qui appartient √† Google, Instagram qui appartient √† Meta, et Facebook, Twitter ou LinkdedIn respectivement les propri√©t√©s de Mark Zuckerberg, d'Elon Musk et de Ryan Roslansky).
En outre, tandis qu'il est devenu tr√®s compliqu√© et co√ªteux d'obtenir de fa√ßon automatis√©e des masses de tweets, extraire des corpus de messages envoy√©s sur le F√©divers se fait au moyen d'API gratuites et abordables sur le plan technique par des non connaisseurs de ce r√©seau.

Contrairement √† Bluesky qui utilise un autre protocole (AT) pouvant potentiellement fonctionner de mani√®re d√©centralis√©e, la d√©centralisation de Mastodon est achev√©e et les instances qui h√©bergent les comptes sont diverses, communiquent entre elles et appartiennent tant√¥t √† des particuliers, tant√¥t √† des instiutions.
Pour h√©berger son compte sur Mastodon, Plazi Verein a choisi l'instance mastodon.green.
Cette instance est administr√©e par Johan Empa, un consultant qui travaille √† r√©duire les √©missions de gaz √† effets de serre du num√©rique dans les organisations et les entreprises europ√©ennes.
Il y a une certaine coh√©rence dans le choix de cette association en faveur de la biodiversit√© √† investir un r√©seau qui promeut et encourage la diversit√© des contenus par la d√©centralisation.
Cette coh√©rence se retrouve dans le choix d'un outil pleinement transparent (si on compare par exemple le fonctionnement de Mastodon √† l'algorithme de X) pour contribuer √† la FAIRisation des donn√©es relatives √† la bibliodiversit√©.

# 1. De Mastodon √† Wikidata

Wikidata est souvent pr√©sent√©e comme la base de donn√©es qui soutend son projet fr√™re, l'encyclop√©die Wikip√©dia.
Wikidata est une ontologie universelle qui a au fil des ans pris une importance croissante dans le monde universitaire.
A titre d'exemple, la base bibliom√©trique *open source* OpenAlex int√®gre depuis deux ans 66 000 concepts provenant de Wikidata.
Les identifiants uniques de Wikidata permettent d'identifier de mani√®re s√ªre et durables les objets de recherche et peuvent √™tre align√©s avec un grand nombre d'autres identifiants issus d'ontologies sp√©cifiques propres √† telle ou telle science.

Wikidata est une base en constant d√©veloppement, et √† la diff√©rence de Plazi qui est une base g√©r√©e par des professionnels de l'information membres d'une association, tout le monde peut y contribuer.
Nous avons constat√© que les derni√®res entr√©es dans la base Plazi n'avaient pas syst√©matiquement fait l'objet de cr√©ations d'√©l√©ments dans Wikidata, ou bien que certaines des informations qui accompagnaient les messages post√©s sur Mastodon comportaient des informations standardis√©es dont on pouvait se servir pour enrichir les √©l√©ments qui auraient d√©j√† √©t√© cr√©√©s dans la base Wikidata.
Nous avons aussi constat√© que les messages envoy√©s par Plazi sur Mastodon qui annon√ßaient la d√©couverte de nouvelles esp√®ces r√©pondaient √† une structure presque intangible qui facilitaient grandement l'extraction de ces informations standardis√©es et leur traitement dans un outil comme R.
Enfin, nous avons r√©cemment d√©couvert deux outils con√ßus pour R qui permettent l'un d'extraire de mani√®re automatis√©e les messages post√©s -dans le vocabulaire de Mastodon on parle de "toots"- sur Mastodon par un utilisateur ou bien sur une instance donn√©e.
Il s'agit du [package R Rtoot](https://www.rdocumentation.org/packages/rtoot/versions/0.3.4), l'autre qui permet d'extraire des donn√©es de Wikidata et dans l'autre sens d'y r√©aliser des modifications de masse.
Ces modifications peuvent √™tre des cr√©ations d'√©l√©ments ou bien des enrichissements ("*statements*").
Ce deuxi√®me package con√ßu pour R est [WikidataR](<https://www.rdocumentation.org/packages/WikidataR/versions/2.3.3>.

Pour les personnes qui ne sont pas famili√®res de la fa√ßon dont sont structur√©es les informations sur Wikidata, voici un exemple :

<img src="images/toot_snake.png" alt="annonce de la d√©couverte d&apos;une nouvelle esp√®ce de serpent au Honduras" width="500">

Cette nouvelle esp√®ce de serpent dont la d√©couverte a √©t√© annonc√©e sur Plazi en janvier 2024 a suscit√© un an plus tard la cr√©ation d'un √©l√©ment dans Wikidata [par un.e internaute](https://www.wikidata.org/w/index.php?title=Q131986719&action=history).
Voil√† comment se pr√©sente cet √©l√©ment :

<img src="images/snake.png" alt="page de l&apos;√©lement dans Wikidata avec les informations suivantes : identifiant (QID), nom, taxon, rang du taxon et taxon parent" width="1000">

Le taxon et le taxon parent sont des *statements*.
Il est possible √† n'importe qui de rajouter des enrichissements en cliquant en bas sur le bouton "add statement".
On pourrait, par exemple, enrichir √† la main cette page avec les informations contenues dans le toot de Plazi :

-   ajouter le lieu de la d√©couverte (Honduras)
-   ajouter l'identifiant de la publication dans laquelle cette esp√®ce est d√©crite
-   ajouter le taxon (qui ne se distingue pas du nom donn√© √† l'esp√®ce et donc au titre ou label de cet √©l√©ment)
-   ajouter le num√©ro de traitement dans la base Plazi, afin -comme dans le cas du DOI- que le lecteur ou la lectrice de cette page puisse acc√©der facilement √† de l'information compl√©mentaire de niveau scientifique.

En revanche, il faudrait un temps consid√©rable pour cr√©er ou enrichir √† la main tous les √©l√©ments dans Wikidata relatifs aux nouvelles d√©couvertes publi√©es sur le compte Mastodon de Plazi (il y en a √† ce jour pr√®s de 200).
Notre but est d'automatiser et ce document expose une m√©thode pour y parvenir.

La m√©thode que nous allons suivre est par cons√©quent la suivante :

1.  Utiliser Rtoot pour extraire tous les *toots* de Plazi annon√ßant l'int√©gration dans la "treatment bnak" de nouvelles esp√®ces d√©couvertes, et importer ces donn√©es dans R.
2.  Traiter ces messages, en utilisant divers packages de R afin de parser le contenu de ces messages pour en extraire les informations susceptibles de permettre la cr√©ation ou l'enrichissement des √©l√©ments Wikidata relatifs √† ces esp√®ces nouvellement apparues.
3.  Mettre en forme ces informations de sorte √† pouvoir au moyen d'une modification de masse ("bulk quick statement") pouvoir les envoyer sur Wikidata. Cela se fera au moyen du package WikidataR et en deux temps : tout d'abord la cr√©ation d'√©l√©ments Wikidata pour les esp√®ces qui n'en disposent pas encore, ensuite, si des √©l√©ments Wikidata correspondent d√©j√† √† ces esp√®res nouvellement trait√©es par Plazi, enrichir ces √©l√©ments en utilisant pour cela le contenu extrait des messages de l'association.

# 2. R√©cup√©ration des toots de Plazi

> **Comp√©tences**
>
> üëâ Utiliser Rtoot pour r√©cup√©rer des messages de Mastodon

Nous pouvons commencer par installer les deux *packages* que nous avons mentionn√©s dans R.
Cela peut se faire √† partir de l'[archive CRAN](https://cran.r-project.org/) :

``` shell

install.packages("rtoot")
install.packages("WikidataR")
# bien respecter la casse dans WikidatR : la premi√®re et derni√®re lettre sont des majuscules
```

Puis nous pouvons importer ces packages dans le script :

```{r}
library(WikidataR)
library(rtoot)

```

David Schoch et Chung-Hong Chan sont √† l‚Äôorigine du package Rtoot con√ßu pour R qu‚Äôils ont pr√©sent√© dans les colonnes de la revue Mobile Media & Communication (@schochSoftwarePresentationRtoot2023b).
Comme une partie des requ√™tes possibles par API n√©cessite une authentification, ce package g√®re cette authentification au moyen d‚Äôun token qui peut √™tre obtenu quand on est connect√© √† son compte.
Il faut donc disposer d'un compte Mastodon pour r√©aliser ce qui suit.
Ce compte peut-√™tre cr√©√© facilement et imm√©diatement sur l'une des plus larges instances, [Mastodon.social](https://mastodon.social/explore) ou bien sur mastodon.green, si vous faites le m√™me choix que Plazi.

Nous avons divis√© le script de l'ensemble de cette op√©ration en plusieurs mini-programmes (dans le vocabulaire de R, on parle de "*chuncks*"), de telle sort qu'il soit plus facile aux personnes qui souhaitent le r√©utiliser de le prendre en main.

Les commandes qui d√©clenchent ce processus d‚Äôauthentification sont indiqu√©es ci-dessous :

```{r authentification √† l\'API r√©cup√©ration de l\'identifiant Plazi, eval=FALSE}
library(rtoot)
auth_setup() # authentification de l'utilisateurice
# la documentation est accesible icihttps://www.rdocumentation.org/packages/rtoot/versions/0.3.4
id <- search_accounts("plazi_species")
```

Ce *chunck* permet de s'authentifier pour acc√©der √† l'API de Mastodon.
Il faut donc que le compte Mastodon de l'utilisateurice soit ouvert.
Un prompt lui est envoy√© pour qu'il.elle ins√®re le nom de l'instance qui h√©berge ce compte (mettre le nom entre guillements : par exemple "mastodon.social") Choisir 2:user Cette action ouvre un popup dans R et dans le navigateur une demande d'autorisation √† accepter pour relier autoriser l'usage de l'API par le compte.
Accepter l'autorisation et copier le jeton obtenu suite √† l'acceptation pour le placer dans le popup c√¥t√© R.
La suite du script peut ensuite √™tre ex√©cut√©e.
Elle vise √† trouver l'idenfiant du compte de Plazi sur Mastodon √† partir de son nom ("plazi_species").
Cet identifiant est n√©cessaire pour les op√©rations suivantes, en particulier la r√©cup√©ration de messages provenant de ce compte.

le *chunk* suivant permet de collecter tous les *toots* envoy√© par Plazi sur Mastodon.

```{r r√©cup√©ration des toots de Plazi, echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}
library(rtoot)
df <- get_account_statuses("109284766172512524", limit = 400L) # chnager la limite po
head(df$content, 1)
```

le pav√© ci-dessus correspond au contenu de l'un des messages envoy√©s par Plazi sur Mastodon.
Ce contenu est presque illisible √† cause de l'abondance des balises html.
On discerne quand m√™me le nom de l'esp√®ce s√©par√© du reste du texte par deux balises p en amont et en aval.
Utiliser ces balises est n√©cessaire pour extraire cette information.
Les autres informations pourront √™tre extraites une fois que le texte des messages aura √©t√© "nettoy√©", d√©barrass√© des √©l√©ments de la syntaxe html.

         
>**Comp√©tences**
>
>üëâ Utiliser des expressions r√©guli√®res pour extraire d'un texte des cha√Ænes de caract√®re
>
>üëâ Supprimer des lignes qui ont une certaine valeur pour une variable donn√©e  


Pour extraire le nom de l'esp√®ce qui se situe entre \</\p\>\<\p\> et \<\p\>\</\p\>, on va utiliser une premi√®re regex ("expression r√©guli√®re") et l'extraction se fera au moyen du package *stringr*.
Les caract√®res extraits du corps du texte, c'est-√†-dire, le nom de l'esp√®ce, vont remplir une nouvelle colonne qui est ajout√©e au moyen du package *dplyr*.

```{r r√©cup√©ration esp√®ce, echo=TRUE, message=FALSE, warning=FALSE}
library(stringr)
library(dplyr)
pattern <- "(?<=</p><p>)(.*?)(?=</p><p>)"
df <- df %>%
  mutate(species_name = str_extract(content, pattern))  
#str_to_sentence(country_discovery$country_of_discovery)
head(df[30:30], 10) # affichage des 10 premiers r√©sutlats au niveau de la colonne 30 qui comporte le nom des esp√®ces 
```

Si l'on observe les 10 premiers r√©sultats de la nouvelle colonne form√©e (la trenti√®me), on constate que certains items ne sont pas des esp√®ces mais comportent du texte encadrant l'identifant de l'esp√®ce dans le registre de Plazi (Treatment suivi d'un lien html qui comporte cet identifiant).
Cette particularit√© concerne les v√©g√©taux, comme dans les exemples suivants :

[https://mastodon.green/\@plazi_species/113917317111853431](https://mastodon.green/@plazi_species/113917317111853431){.uri} [https://mastodon.green/\@plazi_species/113916252425285117](https://mastodon.green/@plazi_species/113916252425285117){.uri}

D'ordinaire la syntaxe des *toots* de Plazi se pr√©sente sous la forme d'un texte qui comporte peu de variations :

| 1\. hashtag Nouvelle esp√®ce | 2\. un nouvel <animal> | 3\. d√©couvert en <pays> | 4\. vient juste de ramper / bondir / voler, etc. jusqu'ici | 5\. <esp√®ce> | 6\. traitement :<identifiant plazi> | 7\. publication : \<doi de la publication |
|:----------|:----------|:----------|:----------|:----------|:----------|:----------|
| #NewSpecies! | New rock gecko from | #thailand | just rocked in: | Cnemaspis enneaporus | Treatment: <https://treatment.plazi.org/id/9A67AE1B-A1A0-55B2-8B6B-EA350C541DA5> | Publication: <https://doi.org/10.3897/zookeys.1226.1> |

Mais pour ces messages o√π l'identifiant de traitement se substitue √† l'esp√®ce, la structure est un peu diff√©rente :

| 1\. hashtag Nouvelle esp√®ce | 2\. <esp√®ce> | 3\. taxon parent | 4\. <pays> d'origine de la d√©couverte | 5\. traitement :<identifiant plazi> | 6\. publication : \<doi de la publication |   |
|:----------|:----------|:----------|:----------|:----------|:----------|:----------|
| #NewSpecies! | Clematis pengii | New ranunculaceae | from #taiwan! | Treatment: treatment.plazi.org/id/B4478577-C711-F54E-FF1F-AE04FE9254DF | Publication: doi.org/10.11646/phytotaxa.662.1.2 |  |

C'est l'identifiant du traitement qui se trouve pris entre les balises d√©j√† √©voqu√©es.
Cette deuxi√®me disposition concerne des esp√®ces v√©g√©tales exclusivement.
Nous d√©cidons pour le moment de ne pas les int√©grer.
Pour cela nous utilisons la fonction filter du package dplyr et une fonction du package dplyr pour supprimer les r√©sultats qui comportent le terme Treatment dans la colonne "Eesp√®ces" (species_name) nouvellement cr√©√©e :

```{r exclusion esp√®ces v√©g√©tales, echo=TRUE, message=FALSE, warning=FALSE}
library(stringr)
library(dplyr)

df <- df %>%
  filter(!str_detect(species_name, 'Treatment:'))
head(df[, 30:30], 10)

```



>**Comp√©tences**
>
>üëâ Utiliser la fonction *html_text* du package *Rvest* pour supprimer des balises html d'un texte
>
>üëâ Utiliser la fonction *str_to_title* de *stringr* pour restaurer la majuscule √† l'initiale des noms de pays
>
>üëâ Utiliser des expressions r√©guli√®res pour isoler du texte qui r√©pond √† une forme rigide (num√©ro de matricule)



Le reste du contenu des messages (colonne "content"), du moins les informations qui nous int√©ressent et dont nous allons nous servir pour enrichir Wikidata, peuvent √™tre isol√©es et extraites sans l'aide de balises html (les regex pourront int√©grer des √©l√©ments du texte).
On se propose donc √† l'√©tape suivante de *parser* tous ces messages pour en supprimer tous les √©l√©ments de formatage en html.
Nous suivons en cela la m√©thode pr√©sent√©e par Stochastics (@stochasticsInteractingMastodonAPIa).
Nous aurons besoin pour r√©aliser cette t√¢che de deux nouveaux packages : *purrr* et *rvest*.

```{r suppression des balises html dans les messages, echo=TRUE, message=FALSE, warning=FALSE}
library(purrr)
library(rvest)
df$content <- map_chr(df$content, function(x) {
  tryCatch({
    read_html(x) %>% html_text()
  }, error = function(e) {
    # renvoie le message original si une erreur est rencontr√©e
    return(x)
  })
})
head(df$content, 4)
```

Voici les 4 premiers √©l√©ments de la liste dans ce nouveau format √©pur√©.
On y voit d√©sormais quand m√™me plus clair, n'est-ce pas ?
On va d√©sormais extraire les noms des pays o√π la d√©couverte a eu lieu.
Ces noms sont cit√©s apr√®s un hashtag et la pr√©position *from*.
Nous construisons notre regex √† partir de ces √©l√©ments :

```{r r√©cup√©ration pays de d√©couverte, echo=TRUE, message=FALSE, warning=FALSE}
library(stringr)
pattern <- "(?<=from #)(\\w+)" # capture le mot qui vient apr√®s l'expression r√©guli√®re "from #"
df <- df %>%
  mutate(
    country_of_discovery = str_extract(content, pattern),
    country_of_discovery = str_to_title(country_of_discovery) #str_to_title permet de restaurer la majuscule √† l'iniotiale des noms de pays.
  )
head(df[, 30:31], 10)
```

Avec la fonction de *stringr* "str_to_title" on a au passage restaur√© la majuscule √† l'initiale du nom de pays qui avait disparu dans le message post√© sur Mastodon.

On va proc√©der de mani√®re similaire pour extraire de la colonne "content" les DOI et les identifiants de la base Plazi.
pour former l'expression r√©guli√®re concernant les DOIs, on va s'aider du suffixe doi.org/ Pour les identifiants Plazi, on observe que leur structure est relativement rigide : m√™me nombre de caract√®res, m√™me mani√®re de scinder ces caract√®res en groupes de caract√®res de 8, 4, 4 et 12 groupes de caract√®res s√©par√©s par des tirets demi-cadratins (-)

```{r r√©cup√©ration DOIs et Plazi IDs, echo=TRUE, message=FALSE, warning=FALSE}
library(stringr)
pattern_doi <- "(?<=doi.org/)(.*?)(?=#)" #regex pour extraire les doi
pattern_plazi <- "[A-Za-z0-9]{8}-[A-Za-z0-9]{4}-[A-Za-z0-9]{4}-[A-Za-z0-9]{4}-[A-Za-z0-9]{12}" # regex pour extraire les identifiants Plazi
df <- df %>%
  mutate(
    doi = str_extract(content, pattern_doi), #cr√©e une colonne pour les DOI
  plazi = str_extract(content, pattern_plazi) #cr√©e une colonne pour les identifiants plazi
    )  
head(df[,30:33], 10)
```

la fonction head(df[,30:33], 10) ne repr√©sente que les 10 premiers √©l√©ments des colonnes 30 √† 34 de notre tableau df (pour *dataframe*) Mais df comprend encore toutes les autres colonnes dont nous n'aurons plus besoin.
Nous pouvons donc nous en d√©barrasser au moyen d'une s√©lection de ces seules colonnes qui sont utiles √† conserver (fonction *select*) Nous ajoutons au passage une colonne *taxon*¬†qui reprend simplement le nom de l'esp√®ce.
En effet, un √©l√©ment Wikidata utilise deux fois le nom de l'esp√®ce : pour le label (= nom de l'√©l√©ment, par d√©faut en anglais, m√™me s'il s'agit ici en fait de latin) et pour le taxon.

```{r s√©lectionner les colonnes importantes pour nous, echo=TRUE, message=FALSE, warning=FALSE}
library(dplyr)
df <- df %>%
  mutate(taxon = paste0(species_name))
df <- select(df, species_name, taxon, country_of_discovery, doi, plazi)
head(df,5)
```

# 3. Pr√©parer les donn√©es en vue de leur envoi vers Wikidata

## 3.1 Qu'est-ce qu'un statement (architecture de la wikibase de Wikidata)

         
>**Comp√©tences** 

>üëâ Comprendre comment sont agenc√©s les items et les propri√©t√©s ainsi que leurs valeurs dans les √©l√©ments de Wikidata  
>
>üëâ Utiliser des identifiants de noms de pays, plut√¥t que des noms de pays pour enrichir Wikidata sans faire d'erreur   
>
>üëâ S'initier √† la structuration des requ√™tes Sparql dans Wikidata 
>
>üëâ Ma√Ætriser l'usage de WikidataR pour l'envoi de requ√™tes en Sparql vers Wikidata
>
>üëâ Fusionner deux tables √† partir d'une colonne pivot identique dans les deux


Wikidata est la plus grande instance existante du logiciel Wikibase.
Wikibase agence les √©l√©ments selon des propri√©t√©s.
On parle de triplet.
Un *statement* est l'action consistant √† ajouter une propri√©t√© et un √©l√©ment relatif √† un √©l√©ment existant.

Par exemple, l'araign√©e *Sinodromus fujianensis* a √©t√© d√©couverte en Chine est un *statement* qui associe √† l'√©l√©ment [Q131545573 "Sinodromus fujianensis"](https://www.wikidata.org/wiki/Q131545573) la propri√©t√© [P189 "Pays de d√©couverte"](https://www.wikidata.org/wiki/Property:P189) qui correspond √† l'√©l√©ment [Q148 "R√©publique Populaire de Chine](https://www.wikidata.org/wiki/Q148). Cette relation illustre bien le mod√®le employ√© par la base de donn√©es :

Q(√©l√©ment) \> P(propri√©t√©) \> Q(√©l√©ment)

Si l'on rapport ce mod√®le aux informations que l'on a d√©j√† extraites des toots de Plazi, nous avons donc le sch√©ma suivant :

| √©l√©ment Q | Propri√©t√© P | √©l√©ment Q ou cha√Æne de caract√®res |
|:----------------------:|:----------------------:|:----------------------:|
| une esp√®ce donn√©e | Len (le nom en langue anglaise) | le nom de l'esp√®ce (une cha√Æne de caract√®res) |
| la m√™me esp√®ce | P189 (a √©t√© d√©couvert dans un pays) | le nom du pays (un √©l√©ment Q) |
| la m√™me esp√®ce | P225 (a pour nom de taxon) | le nom de l'esp√®ce (une cha√Æne de caract√®res) |
| la m√™me esp√®ce | P356 (a pour DOI) | le DOI de l'article dans lequel la d√©couverte a √©t√© consign√©e |
| la m√™me esp√®ce | P1992 (a pour identifiant dans la base Plazi) | l'identifiant de la base Plazi |

Ajouter un *statement*, c'est ajouter une propri√©t√© et une valeur reli√©e √† cette propri√©t√© √† un √©l√©ment.

## 3.2 assigner des √©l√©ments pays √† la liste de pays obtenus via Mastodon

Si l'on veut ajouter √† Wikidata le statement "l'araign√©e Sinodromus Fujinianensis a √©t√© d√©couverte en Chine", il faut prendre en garde a bien relier le bon √©l√©ment (araign√©e Sinodromus) √† la bonne entit√© "R√©publique Populaire de Chine".
Si l'on envoie cette information via une cha√Æne de caract√®res (en conservant le terme "China" pr√©sent dans le message post√© sur Plazi), on pourrait potentiellement relier cette araign√©e √† plusieurs √©l√©ments :

-   1 People's Republic of China (Q148)\
-   2 mainland China (Q19188)\
-   3 porcelaine (en anglais "china") (Q130693)\
-   4 China (Q942154)

Le 1 est ce qui convient le mieux (entit√© territoriale le 2 est un concept politique (la Chine sans Ta√Øwan, ce qui suppose que la Chine soit la Chine "r√©unifi√©e" avec Ta√Øwan) le 3 est un objet en porcelaine (qu'en anglais on nomme effectivement : china) le 4 est une localit√© mexicaine de la province du Nuevo L√©on.

Il y a un risque non n√©gligeable qu'un processus automatis√© d'information en direction de Wikip√©dia traite une cha√Æne de caract√®re comme China de telle sorte qu'un √©l√©ment comme celui de cette municipalit√© mexicaine lui soit associ√©, en lieu et place de celui qui √©tait vis√©.
Pour √©viter cela, nous allons croiser les noms de pays obtenus avec la liste des entit√©s de pays que nous pourrons obtenir de Wikidata.

Pour obtenir cette liste, il est n√©cessaire d'envoyer une requ√™te via WikidataR.

Ce package comporte une fonction de requ√™tage qui proc√®de de la mani√®re suivante :

``` txt

liste_elements <- query_wikidata(' 

placer ici une requ√™te en SPARQL

')
```

Le SPARQL est le langage de requ√™te utilis√© pour interroger des bases comme Wikipedia Nous ne pouvons pas ici nous y attarder, nous nous contenterons de commenter les √©tapes de cette requ√™te :

``` txt
SELECT ?item ?itemLabel           # s√©lectionne les items qui r√©alisent la condition ci-dessous et affiche leur item (QID) et leur label (=nom du pays)
WHERE
{

     ?item wdt:P31 wd:Q6256 .     # les √©l√©ments sont (P31) des pays (Q6256)
           
              SERVICE wikibase:label { bd:serviceParam wikibase:language "en". # saisit les labels anglophones des √©l√©ments en question 
```

Les r√©sultats ne sont pas les identifiants eux-m√™mes mais des URL (<http://www.wikidata.org/entity/Q16> pour le Canada par exemple) qui comportent ces identifiants (Q16).
Nous rajoutons donc un bout de code qui permet de s√©parer le nom de domaine de ces URL des identifiants dont nous avons besoin et obtient de cette mani√®re une nouvelle colonne "qid" qui recense les identifiants de cette liste de pays.

Pour lier ce tableau des pays au pr√©c√©dent tableau, il faut disposer d'une colonne pivot qui doit porter le m√™me nom dans les deux cas.
Ainsi dans le dataframe "countries_id", on peut soit renommer la colonne ItenLabel en "country_of_discovery" qui est d√©j√† pr√©sente dans l'autre tableau (df), soit copier-coller le contenu de la colonne "itemLabel" h√©rit√©e de l'interrogation en Sparql dans une nouvelle colonne qu'on va nommer "country_of_discovery".
Nous avons choisi la deuxi√®me voie.

```{r r√©cup√©rer la liste des identifiants de tous les pays recens√©s dans Wikidata, echo=TRUE, message=FALSE, warning=FALSE}
library(WikidataR)
library(WikidataQueryServiceR)
library(dplyr)
# URL to this query on Wikidata query service
countries_qid <- query_wikidata('
SELECT ?item ?itemLabel
WHERE
{

     ?item wdt:P31 wd:Q6256 .
           
              SERVICE wikibase:label { bd:serviceParam wikibase:language "en". }
}')
qid_no_url <- gsub('[http://www.wikidata.org/entity/]', '', countries_qid$item) # extrait le qid de la colonne item et ajoute une colonne au tableau des pays
countries_qid <- as.data.frame(countries_qid) # fait de countries_qid2 un dataframe afin que mutate puisse fonctionner (ligne suivante)
countries_qid <- countries_qid %>% 
  mutate(
    qid=qid_no_url,
    country_of_discovery = paste(countries_qid$itemLabel)
    )
        
head(countries_qid, 10)
```

Le code suivant fait la jointure entre les deux tableaux sur la base de leur colonne commune "country_of_discovery".
Dor√©navant, dans le document ma√Ætre (d√©sormais appel√© df1), lorsque la d√©couverte a eu lieu au Canada, la valeur Q16 d√©signant le Canada et provenant du tableau issu de Wikidata sera affect√©e √† l'item correspondant (l'esp√®ce d√©couverte au Canada).

```{r jonction des deux tableaux, echo=TRUE, message=FALSE, warning=FALSE}

df1 <- merge(df, countries_qid, by = "country_of_discovery")
df1 <- select(df1, species_name, taxon, qid, country_of_discovery, doi, plazi)
head(df1, 10)

```

# 3.3 Cr√©er ou enrichir


>**Comp√©tences** 
>
>üëâ Avec la fonction find_item() de WikidataR, obtenir √† partir d'un vecteur comportant des identifiants Wikidata la liste des √©l√©ments d√©j√† pr√©sents dans la wikibase  
>  
>üëâ comprendre la structure d'une commande pour √©diter des √©l√©ments Wikidata (fonction write_wikidata()) 
>  
>üëâ r√©diger une commande pour √©diter en masse dans Wikidata  


L'envoi group√© d'informations vers Wikidata n'est pas g√©r√© de la m√™me mani√®re par le package WikidataR selon qu'il s'agisse de cr√©ations (les √©l√©ments ne sont pas d√©j√† pr√©sents dans Wikidata) ou bien d'enrichissements (les √©l√©ments sont pr√©sents, on les enrichit par de nouveaux *statements*) Il va falloir int√©grer cette donn√©e dans notre tableau principal (df1).
Pour cela, pour chaque nom d'esp√®ce que contient le tableau, on va interroger Wikidata pour savoir s'il y a un √©l√©ment correspondant avec la fonction *find_item* du package WikidataR, comme indiqu√© dans le script suivant :

```{r r√©cup√©ration des items qui existent d√©j√† dans Wikidata, echo=TRUE, fig.width=12, message=FALSE, warning=FALSE}

get_qid <- function(name) {
  results <- find_item(name)  # Search for the name in Wikidata
  if (length(results) > 0) {
    return(results[[1]]$id)  # Extract QID of the first result
  } else {
    return(NA)  # Return NA if no result is found
  }
}
df1$QID_items <- sapply(df1$species_name, get_qid)
head(df1, 10)
```

Dans la colonne QID_items, nous avons tant√¥t un identifiant unique d'√©l√©ment de Wikidata (QID), tant√¥t nous n'avons rien.
Lorsque l'identifiant a √©t√© r√©cup√©r√©, c'est donc parce qu'un √©l√©ment existe d√©j√† qu'il conviendra d'enrichir avec de nouvelles propri√©t√©s Lorsque l'identifiant n'existe pas, c'est qu'il n'a pas encore √©t√© cr√©√©, il faudra le cr√©er avec ses propri√©t√©s.

Maintenant que cette nouvelle colonne QID_items est cr√©√©e, avec les identifiants de Wikidata quand les √©l√©ments sont d√©j√† cr√©√©s, vide (NA lorsque ces √©l√©ments n'ont pas encore √©t√© cr√©√©s), on va pouvoir scinder ce tableau en deux : dans le premier (df2), les items qui n'ont pas encore √©t√© cr√©√©s sur Wikidata, dans le second (df3) les items qui ont d√©j√† √©t√© cr√©√©s.
Cr√©ons d√©j√† df2 :

```{r r√©cup√©ration des items qui ne sont pas pr√©sents dans Wikidata, echo=TRUE, message=FALSE, warning=FALSE}
#df2 : items √† cr√©er dans Wikdata
df2  <- df1[is.na(df1$QID_items),] # filter(df1 =="NA") won't do it (see here https://stackoverflow.com/questions/7980622/subset-of-rows-containing-na-missing-values-in-a-chosen-column-of-a-data-frame#7980765)
head(df2, 10)

```

## 3.4 Constitution d'un tableau d'√©l√©ments √† cr√©er

Il est n√©cessaire √† ce stade d'expliquer comment proc√©der l'envoi d'informations vers la wikibase au moyen du package WikidataR.
La fonction dont nous allons nous servir est [write_wikidata()](https://www.rdocumentation.org/packages/WikidataR/versions/2.3.3/topics/write_wikidata).
La syntaxe de base est la suivante :

``` txt

write_wikidata(
  items,                 # items = les identifiants Qids des √©l√©ments √† modifier (cela suppose qu'ils ont d√©j√† √©t√© cr√©√©s)
  properties = NULL,
  values = NULL,
  qual.properties = NULL,
  qual.values = NULL,
  src.properties = NULL,
  src.values = NULL,
  remove = FALSE,
  format = "tibble",
  api.username = NULL,
  api.token = NULL,
  api.format = "v1",
  api.batchname = NULL,
  api.submit = TRUE
)
```

Supposons que nous disposiions d'un dataframe comportant dans une colonne item les identifiants des articles Wikidata de toutes les batailles qui ont eu lieu dans le P√©loponn√®se et dans le m√™me dataframe une colonne value qui porte la date de chacune de ces batailles.
Dans ce cas, la

``` txt

item <- c(dataframe$item)  # liste des √©l√©ments relatifs aux batailles du P√©loponn√®se, par exemple  la bataille d'Aegos Potamos (Q866862) 
property <- "P585"     # Date de l'√©v√©nement
value <- c(dataframe$date_iso)  # date en format ISO

library(WikidataR)
write_wikidata(items = item,
               properties = "P585",
               values = value,
               format = "api",
               api.username = "identifiant_utilisateur", 
               api.token = "mettre ici votre token")
```

Dans ce cas, les items sont pluriels (de m√™me que les dates bien s√ªr), mais la propri√©t√© est unique.
Lorsque nous voulons faire des enrichissements sur plusieurs propri√©t√©s, il est plus commode de faire des ces propri√©t√©s elles-m√™mes un vecteur.
C'est ce que nous faisons dans le *chunck* suivant.

La premi√®re ligne constitue un tableau extrait de df2.
Ce tableau ne comporte que les trois premiers enregistrements de df2.
En effet, pour plus de clart√©, et dans l'optique de refaire cet exercice plusieurs fois, nous ne voulons pas traiter l'ensemble des items disponibles mais seulement une partie d'entre eux.
C'est aussi un principe de pr√©caution que nous appliquons.
Il s'agit de cr√©er des items qui n'existent pas encore dans Wikidata ; n'importe quel utilisateurice peut ajouter ou supprimer des *statements* √† un ou plusieurs √©l√©ments de Wikidata.
Mais supprimer des √©l√©ments n√©cessite un compte d'administrateurice.
Il n'y a pas de retour en arri√®re possible pour le commun des mortels en cas d'erreur (m√™me si les cons√©quences des erreurs sont limit√©es).
Donc autant √™tre prudent et proc√©der avec un √©chantillon de trois items, pour v√©rifier que tout fonctionne et bien comprendre comment cela fonctionne.


>**Comp√©tences** 
>
>üëâ Faire pivoter un tableau avec la fonction *pivot_longer()* du package *Tidyr*   
>
>üëâ r√©cup√©rer le jeton depuis la forge *quickstatement* pour lancer la commande *write_wikidata* du package *WikidataR*



Les √©tapes qui suivent proc√®dent √† la mise en forme de cette information pour qu'elle soit exploitable par la forge de Wikidata, c'est √† dire pour qu'elles soient v√©ritablement envoy√©es vers la base de Wikidata.
Pour ce faire, nous suivons la m√©thode adopt√©e par Katharina Brunner (@brunnerGetYourData).
Katharina Brunner est journaliste de donn√©es attach√©e √† la radio bavaroise et a √©crit sur son blog plusieurs articles int√©ressants sur les projets de la fondation Wikimedia (Wikipedia, Wikimetdia Commons, Wikidata) en lien avec R.

Cette m√©thode comporte deux temps principaux :

1.  renommer les colonnes pour qu'elles correspondent √† des propri√©t√©s enregistr√©es dans Wikidata :

| colonne du tableau | propri√©t√© | commentaire |
|:-----------------------|:-----------------------|:-----------------------|
| CREATE\_ (suivi d'un num√©ro unique) | item | quand l'√©l√©ment n'existe pas, l'item doit √™tre cr√©√© avec la commande CREATE suivi d'un identifiant unique dans la liste des √©l√©ments √† traiter |
| species_name | Len | label (=titre de l'√©l√©ment en anglais) |
| qid | P189 | qid est l'identifiant r√©cup√©r√© de Wikidata correspondant au pays de d√©couverte tel qu'indiqu√© dans Mastodon) |
| taxon | P225 | le taxon est l'√©quivalent du nom de l'esp√®ce r√©cup√©r√© sur Mastodon |
| doi | P356 | le doi est celui de la publication √† laquelle il est fait r√©f√©rence pour chaque d√©couverte sur Mastodon |
| plazi | P1992 | il s'agit du matricule de traitement de l'esp√®ce dans la banque de Plazi |

```{r selection des trois items de t√™te de la liste df2, echo=TRUE, message=FALSE, warning=FALSE}

df2_sub <- slice_head(df2, n = 3, by = NULL)
df2_sub <- df2_sub %>% 
  mutate(
    row_num = row_number(), # cr√©e une colonne comportant le num√©ro de chaque item
    item = paste0("CREATE_",row_num), # cr√©e une colonne item comportant le suffixe CREATE_ suivi du num√©ro de la colonne
    Len = paste0(df2_sub$species_name), # Cr√©e la colonne Len (pour Langue anglaise) qui va servir √† faire le label de l'√©l√©ment Wikidata
    P225 = paste0(df2_sub$taxon),
    P189 = paste0(df2_sub$qid),
    P356 = paste0(df2_sub$doi),
    P1992 = paste0(df2_sub$plazi)
  )
df2_sub <- select(df2_sub, item, species_name, Len, P225, P189, P356, P1992) # s√©lectionne les colonnes √† conserver
head(df2_sub, 10)

```

2.  Faire pivoter le tableau obtenu pour le faire correspondre au format d'import de Wikidata

En effet, tandis que nos candidats √† l'envoi se pr√©sentent horizontalement avec toutes leurs propri√©t√©s sur une seule ligne et autant de colonnes qu'il y a de propri√©t√©s, la forge de Wikidata dans laquelle les imports se font dipose l'information de fa√ßon verticale :

**Dans R :**

| liste des √©l√©ments | liste des items | propri√©t√© 1 | propri√©t√© 2 | propri√©t√© 3 |
|:-------------------|:----------------|:------------|:------------|:------------|
| √©l√©ment 1          | item            | propri√©t√© 1 | propri√©t√© 2 | propri√©t√© 3 |
| √©l√©ment 2          | item            | propri√©t√© 1 | propri√©t√© 2 | propri√©t√© 3 |

**Dans la forge**

| √©l√©ment   | propri√©t√©   |
|:----------|:------------|
| √©l√©m√©nt 1 | item        |
| √©l√©ment 1 | propri√©t√© 1 |
| √©l√©ment 1 | propri√©t√© 2 |
| √©l√©ment 1 | propri√©t√© 3 |
| √©l√©m√©nt 2 | item        |
| √©l√©ment 2 | propri√©t√© 1 |
| √©l√©ment 2 | propri√©t√© 2 |
| √©l√©ment 2 | propri√©t√© 3 |

Pour faire pivoter le tableau que nous avons obtenu et le formater de fa√ßon verticale, nous allons utiliser la fonction *pivot_longer* du package *tidyr* (encore un package √† charger en cours de route)

```{r code de Katharina Brunner, echo=TRUE, message=FALSE, warning=FALSE}
library(tidyr)
library(stringr)

# voir m√©thode pr√©sent√©e par Katharina Brunner : https://katharinabrunner.de/2022/06/wikibase-wikidata-etl-data-import-with-r/ 

import <- df2_sub %>% 
  select(item, 
         matches("^L", ignore.case = FALSE), 
         matches("^D", ignore.case = FALSE), 
         # if there are some Sitelinks to other Wiki pages
         #matches("^S", ignore.case = FALSE), 
         matches("^P", ignore.case = FALSE)) %>% 
  pivot_longer(cols = 2:last_col(), names_to = "property", values_to = "value") %>% 
  # fix helper with two columns referring to the same property
  mutate(property = str_remove(property, "_.*")) %>% 
  filter(!is.na(value)) %>% 
  distinct()

head(import, 10)
```


## 3.5 Utiliser un token de Wikidata pour exporter des √©l√©ments vers la Wikibase

Toute modification de la Wikibase de Wikidata peut √™tre faite √† la main, et comme pour Wikip√©dia, il suffit pour cela d'avoir un ordinateur connect√© √† Internet, pas besoin de compte.
Toutefois, et c'est bien compr√©hensible, les modifications de masse sur Wikidata doivent √™tre tra√ßables.
Un compte est n√©cessaire ; si vous disposez d√©j√† d'un compte utilisateur sur Wikip√©dia, vous pouvez vous en servir pour r√©aliser ce qui suit.
Ce compte vous permettra d'obtenir votre *token* pour faire l'export vers la Wikibase.

Aller sur le [site de Wikidata intitul√© "QuickStatements"](https://quickstatements.toolforge.org/#/user) Se connecter avec son compte Une fois identifi√©, cliquer sur le nom du compte.
Un texte appara√Æt avec au milieu un encadr√© gris (flout√© sur cette image) qui comporte votre *token* personnel (ne le laissez pas tra√Æner dans votre code)

<img src="images/token.png" alt="token disponible pour un utilisateur connect√© sur le site quickstatement" width="1000">

On peut d√©sormais copier ce token et le coller dans le *chunk* ci-dessous :

```{r envoyer vers wikidata, echo=TRUE, eval=FALSE}
library(WikidataR)
write_wikidata(
  items        = import$item,
  properties   = import$property,
  values       = import$value,
  format       = "api",
  api.username = "votre nom d'utilisateur", 
  api.token    = "votre token", #api.token    = mettre ici le token r√©cup√©r√© sur https://quickstatements.toolforge.org/#/user
  )

#results ici : https://quickstatements.toolforge.org/#/batch/243495

```

Lorsque ce script est ex√©cut√©, vous pouvez suivre la progression de l'envoi sur le site QuickStatement.

Voici un rapport concernant trois esp√®ces qui ont √©t√© cr√©√©es sur Wikidata en suivant ce proc√©d√© :

<img src="images/statement.png" alt="console indiquant que les nouveaux √©l√©ments ont bien √©t√© cr√©√©s sur Wikidata avec leurs enrichissements respectifs" width="1000">

## traitement des √©l√©ments qui sont d√©j√† pr√©sents dans Wikidata


>**Comp√©tences** 
>
>üëâ Filtrer un tableau √† partir de la valeur nulle dans une colonne donn√©e
>
<üëâ ins√©rer une liste dans une commande con√ßue pour la fonction *write_wikidata()*


df2 comportait des √©l√©ments qui n'√©taient pas pr√©sents dans Wikidata, nous allons maintenant traiter les esp√®ces qui ont d√©j√† fait l'objet de cr√©ations dans la Wikibase de Wikidata.
Ces items vont √™tre regroup√©s dans un *dataframe* que nous allons appeler **df3**

```{r r√©cup√©ration des items qui sont d√©j√† pr√©sents dans Wikidata, echo=TRUE}
#df3 : items d√©j√† cr√©√©sdans Wikdata
df3  <- df1[!is.na(df1$QID_items),] # filter(df1 =="NA") ne va pas fonctionner dans ce cas (voir ici https://stackoverflow.com/questions/7980622/subset-of-rows-containing-na-missing-values-in-a-chosen-column-of-a-data-frame#7980765)
head(df3, 10)

```

Ce document pr√©sente une m√©thode pour envoyer r√©guli√®rement en groupes des informations provenant du compte Mastodon vers Wikidata.
Il faut dans la mesure du possible ne pas traiter √† chaque fois l'ensemble des nouvelles esp√®ces pr√©sentes dans l'ensemble des *toots* envoy√©s par Plazi sur Mastodon depuis la cr√©ation du compte.
R√©√©crire des *statements*, refaire le travail √† l'identique, n'est pas un probl√®me en soi, puisque les informations ne sont pas dupliqu√©es (il ne peut y avoir dans un √©l√©ment de Wikidata deux fois ou plus la m√™me propri√©t√© avec des valeurs identiques.) Toutefois ne pas traiter d'une fois sur l'autre les m√™mes esp√®ces sans y apporter aucun enrichissement est un gaspillage d'√©lectricit√©.
On va donc exclure dans la liste df3 les √©l√©ments qui ont d√©j√† √©t√© trait√©s par nous.
Nous consid√©rons qu'√† part nous, peu de gens attribuent le pays de d√©couverte (P189) au nouveau taxon.
Lorsque l'√©l√©ment existe et que nous ne sommes pas √† l'origine de sa cr√©ation, nous remarquons que la propri√©t√© P189, n'y est jamais pr√©sente.
Nous consid√®rerons donc que si cette propri√©t√© est d√©j√† pr√©sente, alors c'est que nous avons d√©j√† trait√© l'√©l√©ment.
Nous allons donc supprimer de la liste df3, les √©l√©ments qui dans Wikidata ont d√©j√† la propri√©t√© 189.

Pour cela nous avons besoin de proc√©der en plusieurs √©tapes : 1.
Cr√©er une liste de tous identifiants d'√©l√©ments (QID_items) disponibles dans df3 2.
Ins√©rer cette liste dans une requ√™te Wikidata 3.
Envoyer cette requ√™te et r√©cup√©rer les r√©sultats dans un autre tableau (results) 4.
Dans ce tableau, les QID_items sont automatiquement renomm√©s "items" ; on va les renommer QID_items pour √™tre en mesure de r√©concilier le tableau obtenu par la requ√™te Wikidata et df3 qui comporte √©galement la variable QID_items qui nous servira donc de pivot pour cette fusion.
5.
Exclure du tableau ainsi cr√©√© tous les √©l√©ments qui ont d√©j√† une valeur √† la colonne P189 (d√©j√† trait√©s) C'est ce que font les deux scripts suivants :

```{r echo=TRUE, message=FALSE, warning=FALSE}
library(WikidataR)
library(dplyr)

q_list <- c(df3$QID_items) # constitution d'une liste avec tous les identifiants de pays pr√©sents dans df1

qid_list <- c(paste0("wd:",q_list, collapse = " ")) # ajout du suffixe wd: aux identifiants de pays (pour que le vecteur soit conforme √† la syntaxe de la requ√™te dans Wiidata)


query_sparql <- paste0("SELECT ?item ?locationOfDiscovery 
WHERE {  
    VALUES ?item {", qid_list,"}
      OPTIONAL { ?item wdt:P189 ?locationOfDiscovery. }
SERVICE wikibase:label { bd:serviceParam wikibase:language 'en'. }
}")  # r√©daction de la requ√™te query_sparql en y ins√©rant le vecteur des identifiants de pays avec leur suffixe wd:

results <- query_wikidata(query_sparql) # envoi de la requ√™te vers Wikidata

names(results)[names(results) == 'item'] <- 'QID_items' # renommage de la colonne item en QID_items, nom de colonne commun avec df1 en vue d'une fusion des deux tableaux sur la base de cette colonne

df3 <- merge(df3, results, by = "QID_items") # fusion des deux tableaux
head(df3, 10)

```

filtrage de df3 pour ne retenir que les √©l√©ments qui ne disposent pas d√©j√† de la propri√©t√© P189 :

```{r echo=TRUE, message=FALSE, warning=FALSE}
df3  <- df3 %>%
  filter(is.na(locationOfDiscovery)) # suppression des items qui ont d√©j√† la propri√©t√© P189 (pays de d√©couverte) avec un identifiant de pays

head(df3, 10) # affichage des 10 premiers r√©sultats de la liste 
```

Maintenant que nous avons un tableau complet de tous les √©l√©ments d√©j√† pr√©sents dans Wikidata et susceptibles d'√™tre enrichis par des *statements* suppl√©mentaires (ajout de propri√©t√©s et de leurs valeurs), nous allons constituer un sous-groupe de 3 items pour v√©rifier que notre envoi se d√©roule bien.

Il est temps √† pr√©sent de suivre la m√™me m√©thode de formatage qui a √©t√© d√©j√† suivie plus haut.
A noter que les √©l√©ments n'√©tant pas √† cr√©er, nous n'allons pas utiliser la fonction qui cr√©e pour chaque √©l√©ment candidat √† l'import un identifiant CREATE_N

```{r selection des trois items de t√™te de la liste df3, echo=FALSE}

df3_sub <- slice_head(df3, n = 3, by = NULL)
df3_sub <- df3_sub %>% 
  mutate(
    row_num = row_number(),
    Len = paste0(df3_sub$species_name),
    P189 = paste0(df3_sub$qid),
    P356 = paste0(df3_sub$doi),
    P1992 = paste0(df3_sub$plazi)
  )
df3_sub <- select(df3_sub, QID_items, species_name, Len, P189, P356, P1992)
head(df3_sub, 10)

```

il est temps de faire pivoter cette table :

```{r enrichissement, echo=TRUE, message=FALSE, warning=FALSE}
library(tidyr)
library(stringr)

import2 <- df3_sub %>% 
  select(QID_items, 
         matches("^L", ignore.case = FALSE), 
         matches("^D", ignore.case = FALSE), 
         # if there are some links to other Wiki pages
         #matches("^S", ignore.case = FALSE), 
         matches("^P", ignore.case = FALSE)) %>% 
  pivot_longer(cols = 2:last_col(), names_to = "property", values_to = "value") %>% 
  # fix helper with two columns referring to the same property
  mutate(property = str_remove(property, "_.*")) %>% 
  filter(!is.na(value)) %>% 
  distinct()

print(import2)

```

On peut d√©sormais proc√©der √† l'envoi avec le m√™me compte et le m√™me *token*.

```{r envoyer les enrichissements vers wikidata, echo=TRUE, eval=FALSE}
library(WikidataR)
write_wikidata(
  items        = import2$QID_items,
  properties   = import2$property,
  values       = import2$value,
  format       = "api",
  api.username = "votre nom d'utilisateur", 
  api.token    = "votre jeton l√†", #api.token    = mettre ici le token r√©cup√©r√© sur https://quickstatements.toolforge.org/#/user
  )

```

# Conclusion

A travers cette √©tude, nous allons voulu montrer √† la fois l'int√©r√™t d'un package R pour r√©cup√©rer du contenu publi√© sur Mastodon directement dans R afin d'en faciliter le traitement et en m√™me temps l'utilit√© du package WikidataR con√ßu pour r√©cup√©rer des lots d'information depuis Wikidata, les traiter en local, pour les enrichir avec ses propres donn√©es, et restituer le tout √† sa communaut√© au moyen d'une √©dition de masse.

Nous esp√©rons ainsi contribuer √† la fois √† l'usage de Wikidata comme ontologie pivot dans la FAIRisation des donn√©es de recherche et √† l'analyse des messages √©chang√©s sur le F√©divers qui constituent la seule r√©elle alternative d√©centralis√©e pour toutes celles et ceux, scientifiques ou amateurs et amantrices de sciences en particulier, qui ont du quitter les r√©seaux sociaux propri√©taires devenus des foyers d'extr√©misme et de propagation anti-science.

# Bibliographie
